{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a40e0-f489-47e3-a556-3965bb2788fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import moviepy.editor as mp\n",
    "\n",
    "# Directory containing the videos\n",
    "video_directory = \"F:/thesis/Data/F6/F71\"\n",
    "output_directory = \"F:/thesis/Data/F6/F71_1FPS\"\n",
    "  # Directory to save processed videos\n",
    "fps = 1  # Desired frame rate (frames per second)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through each video file\n",
    "for video_file in os.listdir(video_directory):\n",
    "    video_path = os.path.join(video_directory, video_file)\n",
    "    output_path = os.path.join(output_directory, f\"{video_file}\")  # Output video path\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_file}\")\n",
    "        continue\n",
    "\n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps_original = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Create video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for output video\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Read and save every nth frame\n",
    "    frame_number = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save frame every nth frame\n",
    "        if frame_number % int(fps_original / fps) == 0:\n",
    "            out.write(frame)\n",
    "\n",
    "        frame_number += 1\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Combine video with original audio\n",
    "    audio_clip = mp.AudioFileClip(video_path).subclip().set_duration(frame_number / fps_original)\n",
    "    video_clip = mp.VideoFileClip(output_path)\n",
    "    video_clip = video_clip.set_audio(audio_clip)\n",
    "    video_clip.write_videofile(output_path, codec='libx264', fps=fps, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28d804-c325-4ffa-ae4d-d47007557a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c67e3a9b",
   "metadata": {},
   "source": [
    "# Features from videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb43cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from mtcnn import MTCNN\n",
    "# import torch\n",
    "# from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# # Load the pre-trained FaceNet model\n",
    "# facenet_model = InceptionResnetV1(pretrained='vggface2').eval().cuda()\n",
    "\n",
    "# # Define a function to preprocess and extract features from a video frame\n",
    "# def extract_features(frame):\n",
    "#     # Create an MTCNN detector\n",
    "#     detector = MTCNN()\n",
    "\n",
    "#     # Detect faces in the frame\n",
    "#     results = detector.detect_faces(frame)\n",
    "\n",
    "#     features = []\n",
    "\n",
    "#     for result in results:\n",
    "#         x, y, w, h = result['box']\n",
    "\n",
    "#         # Extract the face region\n",
    "#         face_region = frame[y:y+h, x:x+w]\n",
    "\n",
    "#         # Check if the face region is not empty\n",
    "#         if face_region.shape[0] > 0 and face_region.shape[1] > 0:\n",
    "#             # Define the desired size for FaceNet input (160x160)\n",
    "#             desired_size = (160, 160)\n",
    "\n",
    "#             # Resize the face region using OpenCV\n",
    "#             face_region = cv2.resize(face_region, dsize=desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "#             # Convert the resized face region to a NumPy array\n",
    "#             face_region = np.array(face_region)\n",
    "\n",
    "#             # Normalize pixel values to the range [0, 1] (optional)\n",
    "#             face_region = face_region / 255.0  # You can skip this step if your data is already in the correct range\n",
    "\n",
    "#             # Convert to PyTorch tensor and move to GPU\n",
    "#             face_region = torch.tensor(face_region).permute(2, 0, 1).float().cuda()\n",
    "\n",
    "#             # Expand dimensions to make it compatible with the FaceNet model\n",
    "#             face_region = face_region.unsqueeze(0)\n",
    "\n",
    "#             # Extract features using FaceNet\n",
    "#             with torch.no_grad():\n",
    "#                 face_embedding = facenet_model(face_region).cpu()\n",
    "\n",
    "#             # Append the feature vector to the list\n",
    "#             features.append(face_embedding[0])\n",
    "#         else:\n",
    "#             # Handle the case where the face region is empty or invalid\n",
    "#             print(\"Empty or invalid face region detected\")\n",
    "\n",
    "#     return features\n",
    "\n",
    "\n",
    "# # Process each video and extract features (one frame per second)\n",
    "# video_directory = \"F:/thesis/temp/F0/F64\"\n",
    "# output_file = \"F:/thesis/temp/F0/videos_features_F64.csv\"\n",
    "\n",
    "# video_files = [file for file in os.listdir(video_directory) if file.endswith(\".mp4\")]\n",
    "\n",
    "# all_features = []\n",
    "# count = 0\n",
    "# for video_file in video_files:\n",
    "#     count = count + 1\n",
    "#     print(count)\n",
    "#     cap = cv2.VideoCapture(os.path.join(video_directory, video_file))\n",
    "#     feature_count = 0  # Initialize feature count for each video\n",
    "#     frame_number = 0  # Initialize frame number\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         frame_number += 1\n",
    "\n",
    "#         # Process one frame per second\n",
    "#         if frame_number % cap.get(cv2.CAP_PROP_FPS) == 0:\n",
    "#             # Check if the frame is not empty\n",
    "#             if frame is not None:\n",
    "#                 # Extract features from the frame\n",
    "#                 frame_features = extract_features(frame)\n",
    "#                 v_file = os.path.join(video_directory, video_file)\n",
    "#                 # Append the video file link to each feature row with feature number\n",
    "#                 video_label = f\"{video_file[0]}\"\n",
    "#                 feature_count += 1  # Increment feature count\n",
    "\n",
    "#                 for frame_feature in frame_features:\n",
    "#                     feature_row = [v_file, feature_count, video_label] + frame_feature.tolist()\n",
    "#                     # Add file path and file name\n",
    "#                     all_features.append(feature_row)\n",
    "#             else:\n",
    "#                 # Handle the case where the frame is empty or invalid\n",
    "#                 print(\"Empty or invalid frame detected\")\n",
    "\n",
    "#     cap.release()\n",
    "\n",
    "# # Convert the features to a DataFrame and save to a CSV file\n",
    "# feature_columns = [\"Link\", \"Frame\", \"Label\"] + [f\"Feature_{i}\" for i in range(len(all_features[0]) - 3)]\n",
    "# features_df = pd.DataFrame(all_features, columns=feature_columns)\n",
    "# features_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d0a18-1681-47fe-af25-8a245dbdbdf5",
   "metadata": {},
   "source": [
    "### Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3cd1f-1289-4fe0-a37c-f866e45ed6a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "# Load the pre-trained FaceNet model\n",
    "facenet_model = InceptionResnetV1(pretrained='vggface2').eval().cuda()\n",
    "\n",
    "# Function to extract face embeddings from a frame\n",
    "def extract_face_embeddings(frame, device):\n",
    "    try:\n",
    "        # Create an MTCNN detector\n",
    "        detector = MTCNN()\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        results = detector.detect_faces(frame)\n",
    "\n",
    "        embeddings = []\n",
    "\n",
    "        for result in results:\n",
    "            x, y, w, h = result['box']\n",
    "\n",
    "            # Extract the face region\n",
    "            face_region = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Check if the face region is not empty\n",
    "            if face_region.shape[0] > 0 and face_region.shape[1] > 0:\n",
    "                # Define the desired size for FaceNet input (160x160)\n",
    "                desired_size = (160, 160)\n",
    "\n",
    "                # Resize the face region using OpenCV\n",
    "                face_region = cv2.resize(face_region, dsize=desired_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Convert the resized face region to a NumPy array\n",
    "                face_region = np.array(face_region)\n",
    "\n",
    "                # Normalize pixel values to the range [0, 1]\n",
    "                face_region = face_region / 255.0\n",
    "\n",
    "                # Convert to PyTorch tensor and move to GPU\n",
    "                face_region = torch.tensor(face_region).permute(2, 0, 1).float().to(device)\n",
    "\n",
    "                # Expand dimensions to make it compatible with the FaceNet model\n",
    "                face_region = face_region.unsqueeze(0)\n",
    "\n",
    "                # Extract face embeddings using FaceNet\n",
    "                with torch.no_grad():\n",
    "                    face_embedding = facenet_model(face_region).cpu()\n",
    "\n",
    "                # Append the face embeddings to the list\n",
    "                embeddings.append(face_embedding[0].numpy())\n",
    "            else:\n",
    "                # Handle the case where the face region is empty or invalid\n",
    "                print(\"Empty or invalid face region detected\")\n",
    "\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting face embeddings: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Process each video and extract face embeddings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Process each video and extract face embeddings\n",
    "video_directory = \"F:/thesis/Data/F6/F71_1FPS\"\n",
    "output_file = \"F:/thesis/Data/F6/videos_face_embeddings_71fps.csv\"\n",
    "\n",
    "video_files = [file for file in os.listdir(video_directory) if file.endswith(\".mp4\")]\n",
    "\n",
    "all_features = []\n",
    "count = 0\n",
    "for video_file in video_files:\n",
    "    frame_number = 0\n",
    "    cap = cv2.VideoCapture(os.path.join(video_directory, video_file))\n",
    "    count = count + 1\n",
    "    print(count)\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Check if the frame is not empty and it's time to process a frame\n",
    "        if frame is not None and frame_number % frame_rate == 0:\n",
    "            # Extract face embeddings from the frame\n",
    "            face_embeddings = extract_face_embeddings(frame, device)\n",
    "            v_file = os.path.join(video_directory, video_file)\n",
    "            video_label = f\"{video_file[0]}\"\n",
    "            frame_number += 1\n",
    "            if face_embeddings is not None:\n",
    "                # Append the video file link and face embeddings\n",
    "                for face_embedding in face_embeddings:\n",
    "                    feature_row = [v_file, frame_number, video_label] + face_embedding.tolist()\n",
    "                    all_features.append(feature_row)\n",
    "        \n",
    "        else:\n",
    "            # Handle the case where the frame is empty or invalid\n",
    "            print(\"Empty or invalid frame detected\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Convert the features to a DataFrame and save to a CSV file\n",
    "embedding_columns = [\"Link\", \"Frame\", \"Label\"] + [f\"Embedding_{i}\" for i in range(len(all_features[0]) - 3)]\n",
    "embedding_df = pd.DataFrame(all_features, columns=embedding_columns)\n",
    "embedding_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d516c62-21cd-41fb-b234-bd21ce53b12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0cf17-99b5-4e88-8491-977f15a91804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce03716-68d7-4eee-a1c8-2f80d0dad542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing your video files\n",
    "video_folder = \"F:/thesis/temp/F5/v\"\n",
    "\n",
    "\n",
    "# Iterate over all video files in the folder\n",
    "for video_file in os.listdir(video_folder):\n",
    "    if video_file.endswith(\".mp4\"):  # Assuming your videos have the .mp4 extension\n",
    "        video_path = os.path.join(video_folder, video_file)\n",
    "\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get the frames per second (fps) of the video\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        # Print or store the fps for each video\n",
    "        #print(f\"Video: {video_file}, Frames per Second (fps): {fps}\")\n",
    "        print(f\"Frames per Second (fps): {fps}\")\n",
    "\n",
    "        # Release the video capture object\n",
    "        cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6812c9-fc8b-4062-93fd-45497d2fe89d",
   "metadata": {},
   "source": [
    "# Testing Videos Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56017837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define the NeuralNet class\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load the features CSV file with the provided header\n",
    "# header = [\"Link\", \"Label\"] + [f\"Feature_{i}\" for i in range(512)]\n",
    "features_df = pd.read_csv(\"F:/thesis/Features/Final/Updated/Video_features_final.csv\")\n",
    "audio_features = pd.read_csv(\"F:/thesis/Features/Final/Updated/Audio_Features_final_wave2vec.csv\")\n",
    "\n",
    "Y_VF = audio_features[\"Label\"].values  # Emotion label\n",
    "\n",
    "# Group by 'Counter' and calculate mean\n",
    "X_VF = features_df.drop(columns=[\"Link\", \"Label\"])\n",
    "X_VF = X_VF.groupby('Counter').mean().reset_index()\n",
    "\n",
    "# Convert DataFrame to numpy array\n",
    "X_VF_np = X_VF.iloc[:, 1:].values\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the string labels to integer labels\n",
    "Y_VF = encoder.fit_transform(audio_features[\"Label\"].values)\n",
    "\n",
    "# Convert numpy array to PyTorch tensor\n",
    "X_VF = torch.tensor(X_VF_np, dtype=torch.float32)\n",
    "Y_VF = torch.tensor(Y_VF, dtype=torch.long)\n",
    "\n",
    "# No need for train_test_split, split directly with PyTorch\n",
    "test_size = 0.2\n",
    "num_samples = len(X_VF)\n",
    "num_test_samples = int(test_size * num_samples)\n",
    "num_train_samples = num_samples - num_test_samples\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test = X_VF[:num_train_samples], X_VF[num_train_samples:]\n",
    "y_train, y_test = Y_VF[:num_train_samples], Y_VF[num_train_samples:]\n",
    "\n",
    "# Move data and model to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train, X_test = X_train.to(device), X_test.to(device)\n",
    "y_train, y_test = y_train.to(device), y_test.to(device)\n",
    "\n",
    "# Build a simple neural network model using PyTorch\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(np.unique(Y_VF))\n",
    "model = NeuralNet(input_size, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == y_test).sum().item()\n",
    "    total = y_test.size(0)\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Test accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Use the same encoder instance for inverse transformation\n",
    "predicted_labels = encoder.inverse_transform(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47137de5-f005-4c6a-84c4-919a10dbc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_VF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5016409-a4ba-432c-aa53-5b2e0134b32b",
   "metadata": {},
   "source": [
    "#### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3cc30-6712-4a88-a575-2d31a676cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create MFCCs from audio\n",
    "def create_mfcc(audio_file):\n",
    "    # Load the audio file\n",
    "    x, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    # Compute MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=x, sr=sr)\n",
    "\n",
    "    return mfcc\n",
    "\n",
    "# Function to process all audio files in a folder and its subfolders\n",
    "def process_audio_folder(folder_path):\n",
    "    all_features = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp3\"):  # You can adjust the file extension as needed\n",
    "                audio_file = os.path.join(root, file)\n",
    "                mfcc = create_mfcc(audio_file)\n",
    "                # Flatten the MFCC matrix\n",
    "                mfcc_flat = mfcc.flatten()\n",
    "                # Storing link, label, and flattened MFCCs in a list\n",
    "                all_features.append([audio_file, file[0]] + mfcc_flat.tolist())\n",
    "\n",
    "    return all_features\n",
    "\n",
    "# Function to save features to a CSV file\n",
    "def save_to_csv(data, output_file, column_names):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # Writing header to the CSV file\n",
    "        csv_writer.writerow(column_names)\n",
    "        # Writing data to the CSV file\n",
    "        csv_writer.writerows(data)\n",
    "\n",
    "# Example usage:\n",
    "audio_folder = \"F:/thesis/temp/F0/F\"\n",
    "output_file = \"F:/thesis/temp/F5/MFCC/features_5.csv\"\n",
    "all_features = process_audio_folder(audio_folder)\n",
    "# Generate column names including MFCC indices\n",
    "column_names = [\"Link\", \"Label\"] + [f\"MFCC_{i}\" for i in range(1, len(all_features[0]) - 1)]\n",
    "save_to_csv(all_features, output_file, column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac921f",
   "metadata": {},
   "source": [
    "# Speech Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592360f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define a function to extract audio features from an audio file\n",
    "def extract_audio_features(audio_file):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file)\n",
    "\n",
    "    # Extract audio features\n",
    "    features = []\n",
    "\n",
    "    # Tempo and beat features\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    features.append(tempo)\n",
    "\n",
    "    # Spectral features\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr))\n",
    "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "    features.extend([spectral_centroid, spectral_bandwidth, spectral_contrast, spectral_rolloff])\n",
    "\n",
    "    # MFCC (Mel-frequency cepstral coefficients) features\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13), axis=1)\n",
    "    features.extend(mfccs)\n",
    "\n",
    "    # Zero-crossing rate\n",
    "    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "    features.append(zero_crossing_rate)\n",
    "\n",
    "    # Chroma feature\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))\n",
    "    features.append(chroma)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to process all audio files in a folder and its subfolders\n",
    "def process_audio_folder(folder_path):\n",
    "    all_features = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp3\"):  # You can adjust the file extension as needed\n",
    "                audio_file = os.path.join(root, file)\n",
    "                features = extract_audio_features(audio_file)\n",
    "                # Include the file path and name in the CSV\n",
    "                all_features.append([audio_file, file[0]] + features)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "# Function to save features to a CSV file\n",
    "def save_to_csv(data, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"Link\", \"Label\", \"Tempo\", \"SpectralCentroid\", \"SpectralBandwidth\", \"SpectralContrast\", \"SpectralRolloff\", \"MFCC1\", \"MFCC2\", \"MFCC3\", \"MFCC4\", \"MFCC5\", \"MFCC6\", \"MFCC7\", \"MFCC8\", \"MFCC9\", \"MFCC10\", \"MFCC11\", \"MFCC12\", \"MFCC13\", \"ZeroCrossingRate\", \"Chroma\"])\n",
    "        csv_writer.writerows(data)\n",
    "\n",
    "audio_directory = \"F:/thesis/temp/F0/F64/Audio\"\n",
    "output_file = \"F:/thesis/temp/F0/audio_features_F64.csv\"\n",
    "all_features = process_audio_folder(audio_directory)\n",
    "\n",
    "# Save the audio features to a CSV file\n",
    "save_to_csv(all_features, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791616c-2679-42e7-8d00-fd1f6da1eaa2",
   "metadata": {},
   "source": [
    "### Speech Features from Wav2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48d1047-256e-40e0-9aeb-931e6171721f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\thesis\\thesisEnviroment\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:55: FutureWarning: Loading a tokenizer inside Wav2Vec2Processor from a config that does not include a `tokenizer_class` attribute is deprecated and will be removed in v5. Please add `'tokenizer_class': 'Wav2Vec2CTCTokenizer'` attribute to either your `config.json` or `tokenizer_config.json` file to suppress this warning: \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'facebook/wav2vec2-xls-r-2b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/wav2vec2-xls-r-2b' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mF:\\thesis\\thesisEnviroment\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:53\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[1;32mF:\\thesis\\thesisEnviroment\\lib\\site-packages\\transformers\\processing_utils.py:892\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[0;32m    890\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m--> 892\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    893\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mF:\\thesis\\thesisEnviroment\\lib\\site-packages\\transformers\\processing_utils.py:938\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    936\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[1;32m--> 938\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(attribute_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mF:\\thesis\\thesisEnviroment\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:919\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mF:\\thesis\\thesisEnviroment\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2255\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2258\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2259\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2260\u001b[0m     )\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'facebook/wav2vec2-xls-r-2b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/wav2vec2-xls-r-2b' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the Wav2Vec2 processor and model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#model_name = 'kingabzpro/wav2vec2-large-xls-r-300m-Urdu'\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-xls-r-2b\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 13\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2Processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2Vec2Model\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Define a function to extract audio features from an audio file\u001b[39;00m\n",
      "File \u001b[1;32mF:\\thesis\\thesisEnviroment\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:65\u001b[0m, in \u001b[0;36mWav2Vec2Processor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a tokenizer inside \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from a config that does not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m include a `tokenizer_class` attribute is deprecated and will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     64\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m Wav2Vec2FeatureExtractor\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 65\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Wav2Vec2CTCTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(feature_extractor\u001b[38;5;241m=\u001b[39mfeature_extractor, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32mF:\\thesis\\thesisEnviroment\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2255\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2252\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2258\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2259\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2260\u001b[0m     )\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'facebook/wav2vec2-xls-r-2b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/wav2vec2-xls-r-2b' is the correct path to a directory containing all relevant files for a Wav2Vec2CTCTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import librosa\n",
    "\n",
    "# Load the Wav2Vec2 processor and model\n",
    "model_name = 'kingabzpro/wav2vec2-large-xls-r-300m-Urdu'\n",
    "\n",
    " \n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Define a function to extract audio features from an audio file\n",
    "def extract_audio_features(audio_file):\n",
    "    # Load the audio file with librosa\n",
    "    y, sr = librosa.load(audio_file,sr=16000)\n",
    "\n",
    "    # Process the audio file with Wav2Vec2 processor\n",
    "    inputs = processor(y, return_tensors=\"pt\", padding=\"longest\", sampling_rate=sr)\n",
    "\n",
    "    # Get the model embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**inputs)\n",
    "\n",
    "    # Use the output embeddings as features\n",
    "    features = model_output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    return features.tolist()\n",
    "\n",
    "# Function to process all audio files in a folder and its subfolders\n",
    "def process_audio_folder(folder_path):\n",
    "    all_features = []\n",
    "    count=0\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp3\"):\n",
    "                audio_file = os.path.join(root, file)\n",
    "                features = extract_audio_features(audio_file)\n",
    "                # Include the file path and name in the CSV\n",
    "                count = count + 1\n",
    "               # print(count)\n",
    "                all_features.append([audio_file, file[0]] + features)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "# Function to save features to a CSV file\n",
    "def save_to_csv(data, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"Link\", \"Label\"] + [f\"Feature_{i}\" for i in range(len(data[0]) - 2)])\n",
    "        csv_writer.writerows(data)\n",
    "\n",
    "# Specify the directory containing audio files and the output CSV file\n",
    "audio_directory = \"F:/thesis/Data/F6/F71/Audio\"\n",
    "output_file = \"F:/thesis/Data/F6/Audio Features/audio_features_F71.csv\"\n",
    "\n",
    "# Process audio files and save features to CSV\n",
    "all_features = process_audio_folder(audio_directory)\n",
    "save_to_csv(all_features, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dd0d3-e356-44d7-b2d3-d5351ccb759c",
   "metadata": {},
   "source": [
    "# Testing of Speech Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98b929c-6d2c-4494-aef7-8328a5a817c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adil Majeed\\AppData\\Local\\Temp\\ipykernel_17912\\2478055851.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  sequences = torch.Tensor(sequences).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5502421307506054\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the data from your CSV file\n",
    "data = pd.read_csv(\"F:/thesis/Features/Final/Updated/Audio_Features_final_wave2vec.csv\")\n",
    "Y_AF = data[\"Label\"]\n",
    "# Split data into features (X) and labels (y)\n",
    "X_AF = data.drop(columns=[\"Link\", \"Label\"])\n",
    "\n",
    "\n",
    "# Use LabelEncoder to convert string labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y_AF = label_encoder.fit_transform(Y_AF)\n",
    "\n",
    "# Specify the sequence length (adjust as needed)\n",
    "sequence_length = 20\n",
    "\n",
    "# Create sequences of the desired length\n",
    "sequences = []\n",
    "labels = []\n",
    "for i in range(len(X_AF) - sequence_length + 1):\n",
    "    sequence = X_AF.iloc[i:i+sequence_length].values  # Extract a sequence of features\n",
    "    label = Y_AF[i + sequence_length - 1]  # Use the label of the last item in the sequence\n",
    "    sequences.append(sequence)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert data to PyTorch tensors and move them to the GPU\n",
    "sequences = torch.Tensor(sequences).to(device)\n",
    "labels = torch.LongTensor(labels).to(device)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataLoader for the training set (optional but useful for mini-batch training)\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_dataset = TensorDataset(X_train_A, y_train_A)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define an LSTM-based model\n",
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "        return out\n",
    "\n",
    "# Define the LSTM model hyperparameters\n",
    "input_size = X_train_A.shape[2]  # Input size based on the number of features in each time step\n",
    "hidden_size = 64\n",
    "num_layers = 2  # You can adjust this as needed\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Initialize the model and move it to the GPU\n",
    "model = EmotionLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_A)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Move the predictions to the CPU and convert them to a NumPy array\n",
    "predicted = predicted.cpu().numpy()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_A.cpu().numpy(), predicted)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd142e4",
   "metadata": {},
   "source": [
    "# Features From Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b92b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer for Urdu\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# Define a function to extract features from Urdu text\n",
    "def extract_features(text):\n",
    "    if pd.notna(text):  # Check if the text is not missing or NaN\n",
    "        # Tokenize the text and add special tokens [CLS] and [SEP]\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "\n",
    "        # Convert the input to a PyTorch tensor and move to GPU\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)  # Batch size of 1\n",
    "\n",
    "        # Pass the input through the BERT model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "        # Extract the features (output embeddings of the [CLS] token)\n",
    "        features = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "        return features\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Load your Excel file with text and labels\n",
    "input_excel_file = 'F:/thesis/Data/F6/transcriptions.xlsx'\n",
    "df = pd.read_excel(input_excel_file)  # Use pd.read_excel() to read from an Excel file\n",
    "\n",
    "# Extract features from the Urdu texts and preserve labels\n",
    "all_features = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row[\"Transcription\"]  # Assuming the column name for text is \"Transcription\"\n",
    "    label = row[\"Label\"]  # Assuming the column name for labels is \"Label\"\n",
    "    \n",
    "    print(f\"Processing text: {text}\")\n",
    "    \n",
    "    text_features = extract_features(text)\n",
    "    \n",
    "    if text_features is not None:\n",
    "        # Append both the text features and the label\n",
    "        all_features.append((text_features, label))\n",
    "    else:\n",
    "        print(f\"Text excluded: {text}\")\n",
    "\n",
    "# Flatten the nested arrays\n",
    "all_features = [(features.flatten(), label) for features, label in all_features]\n",
    "\n",
    "# Convert the features to a DataFrame\n",
    "feature_columns = [f\"Feature_{i}\" for i in range(all_features[0][0].shape[0])]\n",
    "features_df = pd.DataFrame([features for features, label in all_features], columns=feature_columns)\n",
    "\n",
    "# Add a column for the labels\n",
    "features_df[\"Label\"] = [label for _, label in all_features]\n",
    "\n",
    "# Save the features to a CSV file (or you can save it to an Excel file using to_excel)\n",
    "output_file = \"F:/thesis/Data/F6/Text/urdu_text_features.csv\"\n",
    "features_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Features saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a3e427-793b-434e-93f6-b4eb36f4e6df",
   "metadata": {},
   "source": [
    "### Fine Tune XLMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01dfabe-fa3a-4b86-a9b2-99399973753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLMRobertaTokenizer, AutoModelForSequenceClassification, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "# Load your CSV file\n",
    "input_excel_file = \"F:/thesis/Features/Final/Updated/transcriptions_final_label.xlsx\"\n",
    "df = pd.read_excel(input_excel_file)\n",
    "df = df.drop(columns=[\"Link\"])\n",
    "\n",
    "# Rename your columns if necessary (assuming 'transcription' and 'label' are the headers)\n",
    "df = df[['Transcription', 'Label']]\n",
    "\n",
    "# Convert labels to integers if they are categorical\n",
    "#df['Label'] = pd.Categorical(df['Label']).codes\n",
    "labels = list(set(df['Label']))\n",
    "label_encoding = {}\n",
    "for i in range(len(labels)):\n",
    "  label_encoding[labels[i]] = i\n",
    "label_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3c1bd-d564-48f7-a33c-fe4374bd2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_label_encoding = {v: k for k, v in label_encoding.items()}\n",
    "rev_label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec0fed-607b-4869-b4a9-2b10a01ab6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7544c-0d40-4993-97b5-d6e27fd85843",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = train_df.replace({'Label': label_encoding})\n",
    "data_test = val_df.replace({'Label': label_encoding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93757d81-08e6-475d-955a-90ba9d57b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7910a23-d199-45ab-8818-4652b54f6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Pandas dataframes into Hugging Face Datasets\n",
    "train_dataset = datasets.Dataset.from_pandas(data_train)\n",
    "val_dataset = datasets.Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d2609-6b25-455f-8f77-941db9aa1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50276f75-6863-4acf-b560-3b2a4f667731",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.DatasetDict()\n",
    "data['train']= train_dataset\n",
    "data['test']= val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ed79c-36d1-429a-bca9-a6552f1ca581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokaneizer_name = \"xlm-roberta-large\"\n",
    "model_name = \"xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokaneizer_name)\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Transcription\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_datasets = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae7c20-a990-4ee1-9de1-6b0129a4b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize the datasets\n",
    "# train_dataset = train_da.map(tokenize_function, batched=True)\n",
    "# val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Set the format for PyTorch tensors\n",
    "# train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"Label\"])\n",
    "# val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6abf28-e774-433b-9105-75d8de0a6249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Load the tokenizer\n",
    "# tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# # Load the model (adjust num_labels to match your task)\n",
    "# model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=df['Label'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c69e6-f2ac-41d7-bce3-3e308ac007f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fb87b-7982-4656-a543-62712e6d156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(rev_label_encoding), id2label=rev_label_encoding, label2id=label_encoding, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264840f4-969f-45c5-ae77-686c8680a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = evaluate.load(\"f1\")\n",
    "metric2 = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a60534-535f-4bb5-9415-5a9b9dc08156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {'f1': metric1.compute(predictions=predictions, references=labels, average='macro')['f1'], 'accuracy': metric2.compute(predictions=predictions, references=labels)['accuracy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a9514-a4fd-4602-bf34-0582f9125017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(output_dir=\"./results\",\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  save_strategy='epoch',\n",
    "                                  num_train_epochs=5,\n",
    "                                  per_device_train_batch_size=2,\n",
    "                                  per_gpu_train_batch_size=2,\n",
    "                                  per_device_eval_batch_size=2,\n",
    "                                  per_gpu_eval_batch_size=2,\n",
    "                                  save_total_limit = 5,\n",
    "                                  metric_for_best_model = 'f1',\n",
    "                                  load_best_model_at_end=True,\n",
    "                                 learning_rate = 5e-5\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de974562-47a1-4083-9f35-7d29d004f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb15c2-2b55-42b3-b368-535aeaa68787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10786dd0-3d84-4417-8bdd-56db07d2fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f86ee0-5d69-4965-b52f-d033239e6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./fine_tuned_xlm_roberta')\n",
    "tokenizer.save_pretrained('./fine_tuned_xlm_roberta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c353206-bfdc-4db5-8317-d3bdddaf8c5d",
   "metadata": {},
   "source": [
    "### XLMR Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc494d52-c7fc-43a0-8e20-c693e097e54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\thesis\\thesisEnviroment\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the pre-trained XLM-RoBERTa model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "model = AutoModel.from_pretrained(\"xlm-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02fef2-7623-49f0-b382-a7dc4763d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "file_path = 'F:/thesis/Features/Final/Updated/transcriptions_final_label.xlsx'  # Replace with your file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Assuming the columns are named 'Link', 'Label', and 'Text'\n",
    "links = df['Link'].tolist()\n",
    "labels = df['Label'].tolist()\n",
    "texts = df['Transcription'].tolist()\n",
    "\n",
    "# Tokenize sentences and extract embeddings\n",
    "def get_embeddings(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    sentence_embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "    return sentence_embedding\n",
    "\n",
    "# Process each text to get embeddings\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    embedding = get_embeddings(text)\n",
    "    embeddings.append(embedding.cpu().numpy())  # Convert to numpy array\n",
    "\n",
    "# Create a DataFrame to store embeddings along with links and labels\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "embeddings_df['Link'] = links\n",
    "embeddings_df['Label'] = labels\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = 'F:/thesis/Data/F6/Text/urdu_embeddings.csv'  # Specify your output file name\n",
    "embeddings_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Embeddings saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c126bda-8a29-4fdf-ba2f-22ef46def104",
   "metadata": {},
   "source": [
    "# Testing of Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbb56a7-ed6e-4bd7-82e1-806bac67b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the features from the Excel file\n",
    "input_file = (\"F:/thesis/Features/Final/Updated/Urdu_embeddings_xlmr_large.csv\")\n",
    "df = pd.read_csv(input_file)\n",
    "df = df.drop(columns=[\"Link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb55520c-1fba-40d8-b229-2b2ab72f3952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Split the data into features (X) and labels (y)\n",
    "#df = df.drop(['Link'],axis = 1)\n",
    "data = df.iloc[::]  # Features\n",
    "Y = df['Label']      # Target variable\n",
    "X = data.drop(['Label'],axis = 1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457ebc69-c633-47c7-b036-8b027db02c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c66e38-a612-4394-88ff-3d3ccd1214a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\thesis\\thesisEnviroment\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:55:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Accuracy: 44.81%\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define and train the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,      # Number of boosting rounds (trees)\n",
    "    tree_method=\"hist\",\n",
    "    learning_rate=0.5,     # Learning rate for boosting\n",
    "    max_depth=6,           # Maximum depth of trees\n",
    "    eval_metric='mlogloss', # Log loss for multi-class classification\n",
    "    use_label_encoder=False # Disable automatic label encoding\n",
    ")\n",
    "\n",
    "# Train the XGBoost classifier on the extracted features and corresponding labels\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f\"XGBoost Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69055d2a-732e-4be6-852a-ae4d56cca492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train an SVM classifier\n",
    "classifier = SVC(kernel='linear', C=1.0, probability=True)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# If you want probability estimates, you can use predict_proba\n",
    "# probabilities = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fc642-3775-4f46-9fce-633584691f43",
   "metadata": {},
   "source": [
    "# Late Fusion with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b71451-5e42-4b8f-9cd8-f401b62db8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define LSTM models for each modality (Video, Audio, Text)\n",
    "\n",
    "class VideoLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(VideoLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from input 'x'\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class AudioLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(AudioLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from input 'x'\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class TextLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(TextLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from input 'x'\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define Late Fusion model with LSTM\n",
    "class LateFusionLSTM(nn.Module):\n",
    "    def __init__(self, video_model, audio_model, text_model, output_size):\n",
    "        super(LateFusionLSTM, self).__init__()\n",
    "        self.video_model = video_model\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.fc = nn.Linear(3 * output_size, output_size)\n",
    "\n",
    "    def forward(self, video_input, audio_input, text_input):\n",
    "        video_output = self.video_model(video_input)\n",
    "        audio_output = self.audio_model(audio_input)\n",
    "        text_output = self.text_model(text_input)\n",
    "\n",
    "        combined = torch.cat((video_output[:, -1, :], audio_output[:, -1, :], text_output[:, -1, :]), dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "\n",
    "# Initialize LSTM models for each modality\n",
    "video_input_size = 100  # Modify this according to your feature size\n",
    "audio_input_size = 20   # Modify this according to your feature size\n",
    "text_input_size = 768   # Modify this according to your feature size\n",
    "hidden_size = 64        # Modify this according to your choice\n",
    "output_size = 5         # Modify this according to the number of classes\n",
    "num_layers = 2          # Modify this according to your choice\n",
    "\n",
    "video_lstm_model = VideoLSTMModel(video_input_size, hidden_size, output_size, num_layers).to(device)\n",
    "audio_lstm_model = AudioLSTMModel(audio_input_size, hidden_size, output_size, num_layers).to(device)\n",
    "text_lstm_model = TextLSTMModel(text_input_size, hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "late_fusion_lstm_model = LateFusionLSTM(video_lstm_model, audio_lstm_model, text_lstm_model, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20476cb-9728-480e-8973-23af53a6f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video, audio, and text features separately\n",
    "video_features = pd.read_csv(\"F:/thesis/Data/Final/Video_Features_final.csv\")\n",
    "audio_features = pd.read_csv(\"F:/thesis/Data/Final/audio_features_final.csv\")\n",
    "text_features = pd.read_excel(\"F:/thesis/Data/Final/urdu_text_features.xlsx\")\n",
    "\n",
    "# Extract labels for each modality\n",
    "video_labels = video_features[\"Label\"]\n",
    "audio_labels = audio_features[\"Label\"]\n",
    "text_labels = text_features[\"Label\"]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "video_features = video_features.drop(columns=[\"Link\", \"Label\"])\n",
    "audio_features = audio_features.drop(columns=[\"Link\", \"Label\"])\n",
    "text_features = text_features.drop(columns=[\"Label\"])\n",
    "\n",
    "# Convert text labels to numerical form using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "text_labels_numeric = label_encoder.fit_transform(text_labels)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors for each modality\n",
    "video_tensor = torch.tensor(video_features.values, dtype=torch.float32)\n",
    "audio_tensor = torch.tensor(audio_features.values, dtype=torch.float32)\n",
    "text_tensor = torch.tensor(text_features.values, dtype=torch.float32)\n",
    "text_labels_tensor = torch.tensor(text_labels_numeric, dtype=torch.long)\n",
    "\n",
    "\n",
    "#video_labels_numeric = label_encoder.fit_transform(video_labels)\n",
    "#audio_labels_numeric = label_encoder.fit_transform(audio_labels)\n",
    "\n",
    "#video_labels_tensor = torch.tensor(video_labels_numeric, dtype=torch.long)\n",
    "#audio_labels_tensor = torch.tensor(audio_labels_numeric, dtype=torch.long)\n",
    "\n",
    "# Combine features and labels into a single dataset\n",
    "#video_dataset = TensorDataset(video_tensor, text_labels_tensor)\n",
    "#audio_dataset = TensorDataset(audio_tensor, text_labels_tensor)\n",
    "#text_dataset = TensorDataset(text_tensor, text_labels_tensor)\n",
    "\n",
    "# Create data loaders for each modality\n",
    "#batch_size = 64\n",
    "#video_loader = DataLoader(video_dataset, batch_size=batch_size, shuffle=True)\n",
    "#audio_loader = DataLoader(audio_dataset, batch_size=batch_size, shuffle=True)\n",
    "#text_loader = DataLoader(text_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b05512f-2607-49f5-b9a3-9166a8eed064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(features, labels, batch_size, shuffle=True):\n",
    "    dataset = TensorDataset(features, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "late_fusion_optimizer = optim.Adam(late_fusion_lstm_model.parameters(), lr=0.001)\n",
    "criterion_combined = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs_late_fusion = 10  # Set the number of epochs as needed\n",
    "\n",
    "# Splitting data into train and validation sets\n",
    "video_train, video_val, audio_train, audio_val, text_train, text_val, label_train, label_val = train_test_split(\n",
    "    video_tensor, audio_tensor, text_tensor, text_labels_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating separate datasets and data loaders for train and validation\n",
    "train_dataset = TensorDataset(video_train, audio_train, text_train, label_train)\n",
    "val_dataset = TensorDataset(video_val, audio_val, text_val, label_val)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs_late_fusion):\n",
    "    late_fusion_lstm_model.train()\n",
    "    \n",
    "    for batch_idx, (video_batch, audio_batch, text_batch, label_batch) in enumerate(train_loader):\n",
    "        # Move each batch to device\n",
    "        video_inputs = video_batch.to(device)\n",
    "        audio_inputs = audio_batch.to(device)\n",
    "        text_inputs = text_batch.to(device)\n",
    "        labels = label_batch.to(device).view(-1)  # Assuming labels are already in the correct shape\n",
    "        \n",
    "        late_fusion_optimizer.zero_grad()\n",
    "        \n",
    "        video_outputs = video_lstm_model(video_inputs)\n",
    "        audio_outputs = audio_lstm_model(audio_inputs)\n",
    "        text_outputs = text_lstm_model(text_inputs)\n",
    "        \n",
    "        # Combine outputs of individual models\n",
    "        combined_outputs = late_fusion_lstm_model(video_outputs, audio_outputs, text_outputs)\n",
    "        \n",
    "        # Calculate loss and optimize\n",
    "        loss_late_fusion = criterion_combined(combined_outputs, labels)\n",
    "        loss_late_fusion.backward()\n",
    "        late_fusion_optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    late_fusion_lstm_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (video_batch, audio_batch, text_batch, label_batch) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            # Move each batch to device\n",
    "            video_inputs = video_batch.to(device)\n",
    "            audio_inputs = audio_batch.to(device)\n",
    "            text_inputs = text_batch.to(device)\n",
    "            labels = label_batch.to(device).view(-1)  # Assuming labels are already in the correct shape\n",
    "            \n",
    "            video_outputs = video_lstm_model(video_inputs)\n",
    "            audio_outputs = audio_lstm_model(audio_inputs)\n",
    "            text_outputs = text_lstm_model(text_inputs)\n",
    "            \n",
    "            # Combine outputs of individual models\n",
    "            combined_outputs = late_fusion_lstm_model(video_outputs, audio_outputs, text_outputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            val_loss += criterion_combined(combined_outputs, labels).item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = combined_outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_late_fusion}] | Validation Loss: {val_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed864f-5625-4acc-b4bf-8e195d67b040",
   "metadata": {},
   "source": [
    "# Testing Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee9827-fd4f-493c-b35b-ba49b597142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define LSTM models for each modality (Video, Audio, Text)\n",
    "class VideoLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(VideoLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from input 'x'\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from input 'x'\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class AudioLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(AudioLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from input 'x'\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class TextLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(TextLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)  # Get the batch size from input 'x'\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define Late Fusion model with LSTM\n",
    "class LateFusionLSTM(nn.Module):\n",
    "    def __init__(self, video_model, audio_model, text_model, output_size):\n",
    "        super(LateFusionLSTM, self).__init__()\n",
    "        self.video_model = video_model\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.fc = nn.Linear(3 * output_size, output_size)\n",
    "\n",
    "    def forward(self, video_input, audio_input, text_input):\n",
    "        video_output = self.video_model(video_input)\n",
    "        audio_output = self.audio_model(audio_input)\n",
    "        text_output = self.text_model(text_input)\n",
    "\n",
    "        combined = torch.cat((video_output[:, -1, :], audio_output[:, -1, :], text_output[:, -1, :]), dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return out\n",
    "\n",
    "# Initialize LSTM models for each modality\n",
    "video_input_size = 100  # Modify this according to your feature size\n",
    "audio_input_size = 20   # Modify this according to your feature size\n",
    "text_input_size = 768   # Modify this according to your feature size\n",
    "hidden_size = 64        # Modify this according to your choice\n",
    "output_size = 5         # Modify this according to the number of classes\n",
    "num_layers = 2          # Modify this according to your choice\n",
    "\n",
    "video_lstm_model = VideoLSTMModel(video_input_size, hidden_size, output_size, num_layers).to(device)\n",
    "audio_lstm_model = AudioLSTMModel(audio_input_size, hidden_size, output_size, num_layers).to(device)\n",
    "text_lstm_model = TextLSTMModel(text_input_size, hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "late_fusion_lstm_model = LateFusionLSTM(video_lstm_model, audio_lstm_model, text_lstm_model, output_size).to(device)\n",
    "\n",
    "# Load video, audio, and text features separately\n",
    "video_features = pd.read_csv(\"F:/thesis/Data/Final/Video_Features_final.csv\")\n",
    "audio_features = pd.read_csv(\"F:/thesis/Data/Final/audio_features_final.csv\")\n",
    "text_features = pd.read_excel(\"F:/thesis/Data/Final/urdu_text_features.xlsx\")\n",
    "\n",
    "# Extract labels for each modality\n",
    "video_labels = video_features[\"Label\"]\n",
    "audio_labels = audio_features[\"Label\"]\n",
    "text_labels = text_features[\"Label\"]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "video_features = video_features.drop(columns=[\"Link\", \"Label\"])\n",
    "audio_features = audio_features.drop(columns=[\"Link\", \"Label\"])\n",
    "text_features = text_features.drop(columns=[\"Label\"])\n",
    "\n",
    "# Convert text labels to numerical form using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "text_labels_numeric = label_encoder.fit_transform(text_labels)\n",
    "\n",
    "# Convert Pandas DataFrames to PyTorch tensors for each modality\n",
    "video_tensor = torch.tensor(video_features.values, dtype=torch.float32)\n",
    "audio_tensor = torch.tensor(audio_features.values, dtype=torch.float32)\n",
    "text_tensor = torch.tensor(text_features.values, dtype=torch.float32)\n",
    "text_labels_tensor = torch.tensor(text_labels_numeric, dtype=torch.long)\n",
    "\n",
    "\n",
    "def create_data_loader(features, labels, batch_size, shuffle=True):\n",
    "    dataset = TensorDataset(features, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "late_fusion_optimizer = optim.Adam(late_fusion_lstm_model.parameters(), lr=0.001)\n",
    "criterion_combined = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs_late_fusion = 10  # Set the number of epochs as needed\n",
    "\n",
    "# Splitting data into train and validation sets\n",
    "video_train, video_val, audio_train, audio_val, text_train, text_val, label_train, label_val = train_test_split(\n",
    "    video_tensor, audio_tensor, text_tensor, text_labels_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating separate datasets and data loaders for train and validation\n",
    "train_dataset = TensorDataset(video_train, audio_train, text_train, label_train)\n",
    "val_dataset = TensorDataset(video_val, audio_val, text_val, label_val)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs_late_fusion):\n",
    "    late_fusion_lstm_model.train()\n",
    "    \n",
    "    for batch_idx, (video_batch, audio_batch, text_batch, label_batch) in enumerate(train_loader):\n",
    "        # Move each batch to device\n",
    "        video_inputs = video_batch.to(device)\n",
    "        audio_inputs = audio_batch.to(device)\n",
    "        text_inputs = text_batch.to(device)\n",
    "        labels = label_batch.to(device).view(-1)  # Assuming labels are already in the correct shape\n",
    "        \n",
    "        late_fusion_optimizer.zero_grad()\n",
    "        \n",
    "        video_outputs = video_lstm_model(video_inputs)\n",
    "        audio_outputs = audio_lstm_model(audio_inputs)\n",
    "        text_outputs = text_lstm_model(text_inputs)\n",
    "        \n",
    "        # Combine outputs of individual models\n",
    "        combined_outputs = late_fusion_lstm_model(video_outputs, audio_outputs, text_outputs)\n",
    "        \n",
    "        # Calculate loss and optimize\n",
    "        loss_late_fusion = criterion_combined(combined_outputs, labels)\n",
    "        loss_late_fusion.backward()\n",
    "        late_fusion_optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    late_fusion_lstm_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (video_batch, audio_batch, text_batch, label_batch) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            # Move each batch to device\n",
    "            video_inputs = video_batch.to(device)\n",
    "            audio_inputs = audio_batch.to(device)\n",
    "            text_inputs = text_batch.to(device)\n",
    "            labels = label_batch.to(device).view(-1)  # Assuming labels are already in the correct shape\n",
    "            \n",
    "            video_outputs = video_lstm_model(video_inputs)\n",
    "            audio_outputs = audio_lstm_model(audio_inputs)\n",
    "            text_outputs = text_lstm_model(text_inputs)\n",
    "            \n",
    "            # Combine outputs of individual models\n",
    "            combined_outputs = late_fusion_lstm_model(video_outputs, audio_outputs, text_outputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            val_loss += criterion_combined(combined_outputs, labels).item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = combined_outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_late_fusion}] | Validation Loss: {val_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab0e72-ced0-4d73-8793-2b389be72eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_inputs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee938d05-3319-4ea5-b432-932ab525f3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73e74b-5f60-4d24-ab4c-b17dd2efb528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the plot_metrics function\n",
    "def plot_metrics(train_losses, test_losses, accuracies):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Test Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracies, label='Test Accuracy', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Test Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Your previous code here..\n",
    "\n",
    "# Plotting Loss and Accuracy\n",
    "plot_metrics(train_losses, test_losses, accuracies)\n",
    "\n",
    "# Calculate F1 Score\n",
    "late_fusion_lstm_model.eval()\n",
    "all_predicted = []\n",
    "all_targets = []\n",
    "\n",
    "for video_inputs, audio_inputs, text_inputs, targets in test_loader:\n",
    "    video_inputs, audio_inputs, text_inputs, targets = (\n",
    "        video_inputs.to(device),\n",
    "        audio_inputs.to(device),\n",
    "        text_inputs.to(device),\n",
    "        targets.to(device),\n",
    "    )\n",
    "    \n",
    "    outputs = late_fusion_lstm_model(video_inputs, audio_inputs, text_inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    all_predicted.extend(predicted.cpu().numpy())\n",
    "    all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum([1 for i, j in zip(all_predicted, all_targets) if i == j]) / len(all_targets)\n",
    "\n",
    "f1 = f1_score(all_targets, all_predicted, average='weighted')\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(all_targets, all_predicted)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Report :\\n\", report)\n",
    "\n",
    "# Confusion Matrix\n",
    "labels_list = label_encoder.classes_\n",
    "conf_matrix = confusion_matrix(all_targets, all_predicted)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels_list, yticklabels=labels_list)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0bb8c4-bf46-4eb6-8a70-b43eb6006936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce1662-d5dd-4304-9ce0-720ea54ef397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840692d-e06f-46a5-97e6-7cb3e4149569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss and Optimizer for Individual Models\n",
    "criterion_video = nn.CrossEntropyLoss()\n",
    "optimizer_video = optim.Adam(video_lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion_audio = nn.CrossEntropyLoss()\n",
    "optimizer_audio = optim.Adam(audio_lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion_text = nn.CrossEntropyLoss()\n",
    "optimizer_text = optim.Adam(text_lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loops for Individual Models\n",
    "num_epochs = 10  # Set the number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Loop for Video Model\n",
    "    video_lstm_model.train()\n",
    "    for video_inputs, video_labels in video_loader:\n",
    "        optimizer_video.zero_grad()\n",
    "        video_outputs = video_lstm_model(video_inputs)\n",
    "        loss_video = criterion_video(video_outputs.squeeze(), video_labels)\n",
    "        loss_video.backward()\n",
    "        optimizer_video.step()\n",
    "\n",
    "    # Training Loop for Audio Model\n",
    "    audio_lstm_model.train()\n",
    "    for audio_inputs, audio_labels in audio_loader:\n",
    "        optimizer_audio.zero_grad()\n",
    "        audio_outputs = audio_lstm_model(audio_inputs)\n",
    "        loss_audio = criterion_audio(audio_outputs.squeeze(), audio_labels)\n",
    "        loss_audio.backward()\n",
    "        optimizer_audio.step()\n",
    "\n",
    "    # Training Loop for Text Model\n",
    "    text_lstm_model.train()\n",
    "    for text_inputs, text_labels in text_loader:\n",
    "        optimizer_text.zero_grad()\n",
    "        text_outputs = text_lstm_model(text_inputs)\n",
    "        loss_text = criterion_text(text_outputs.squeeze(), text_labels)\n",
    "        loss_text.backward()\n",
    "        optimizer_text.step()\n",
    "\n",
    "# Training the Late Fusion Model\n",
    "late_fusion_optimizer = optim.Adam(late_fusion_lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs_late_fusion = 10  # Set the number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs_late_fusion):\n",
    "    late_fusion_lstm_model.train()\n",
    "    for video_inputs, audio_inputs, text_inputs, labels in zip(video_loader, audio_loader, text_loader, label_loader):\n",
    "        optimizer_late_fusion.zero_grad()\n",
    "        video_outputs = video_lstm_model(video_inputs)\n",
    "        audio_outputs = audio_lstm_model(audio_inputs)\n",
    "        text_outputs = text_lstm_model(text_inputs)\n",
    "        \n",
    "        # Combine outputs of individual models\n",
    "        combined_outputs = late_fusion_lstm_model(video_outputs, audio_outputs, text_outputs)\n",
    "        \n",
    "        # Calculate loss and optimize\n",
    "        loss_late_fusion = criterion_combined(combined_outputs.squeeze(), labels)\n",
    "        loss_late_fusion.backward()\n",
    "        optimizer_late_fusion.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
