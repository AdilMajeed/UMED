{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bd187b-074c-4002-bdcd-609e8486f25e",
   "metadata": {},
   "source": [
    "### Videos Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e0e8d-a8d3-415e-8d36-2bdf34c6beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchinfo\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Positional Encoding class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len=15):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register as buffer so it doesn't get updated by gradients\n",
    "        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure positional encoding is on the same device as x\n",
    "        self.pe = self.pe.to(x.device)\n",
    "\n",
    "        # Add positional encoding to the input (broadcasting across the batch)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ab0ee-4b90-4654-84f3-ed1d53efdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# VideoEmotionTransformer class\n",
    "class VideoEmotionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, num_classes, dropout, max_len):\n",
    "        super(VideoEmotionTransformer, self).__init__()\n",
    "\n",
    "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim, max_len)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, \n",
    "            nhead=num_heads,\n",
    "            dropout = dropout \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, mask=None, extract_embeddings=False):\n",
    "        # Project input to hidden dimension\n",
    "        x = self.fc_in(x)  # Shape: (batch_size, num_frames, hidden_dim)\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        x = self.positional_encoding(x)  # Shape: (batch_size, num_frames, hidden_dim)\n",
    "        \n",
    "        # Permute for transformer input (transformers expect (sequence_length, batch_size, feature_dim))\n",
    "        x = x.permute(1, 0, 2)  # Shape: (num_frames, batch_size, hidden_dim)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        output = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Permute back to (batch_size, num_frames, hidden_dim)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        \n",
    "        if extract_embeddings:\n",
    "            return output.mean(dim=1)  # Return embeddings for extraction\n",
    "\n",
    "        output = output.mean(dim=1)  # Average over the time dimension (num_frames) for final classification\n",
    "        # Pass through the output layer\n",
    "        output = self.fc_out(output)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a02f746-0493-45d0-a75e-9ef3a96aeff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask generation for padded sequences\n",
    "def generate_padding_mask(batch, pad_token=0):\n",
    "    mask = (batch.sum(dim=-1) == pad_token)  # Shape: (batch_size, num_frames)\n",
    "    return mask  # True for padded frames\n",
    "\n",
    "\n",
    "# Data Processing\n",
    "def preprocess_data(csv_path, max_len=15, input_dim=512):\n",
    "    # Load the CSV data\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Assume 'Counter' column indicates frames and group the features\n",
    "    feature_columns = [col for col in data.columns if col.startswith('Embedding')]\n",
    "    \n",
    "    # Group by 'Counter' to get the frames of each video\n",
    "    grouped = data.groupby('Counter')\n",
    "    video_features, video_labels = [], []\n",
    "    \n",
    "    for counter, group in grouped:\n",
    "        features = group[feature_columns].values  # Features for the frames\n",
    "        label = group['Label'].iloc[0]  # Assuming all frames of a video share the same label\n",
    "        \n",
    "        # Apply padding if the number of frames < max_len\n",
    "        if len(features) < max_len:\n",
    "            pad_length = max_len - len(features)\n",
    "            padding = torch.zeros((pad_length, input_dim))  # Padding with zeros\n",
    "            features = torch.cat((torch.tensor(features), padding), dim=0)\n",
    "        else:\n",
    "            features = torch.tensor(features[:max_len])  # Truncate to max_len\n",
    "\n",
    "        video_features.append(features)\n",
    "        video_labels.append(label)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    video_features = torch.stack(video_features)\n",
    "    \n",
    "    # Convert video labels into a tensor (after label encoding)\n",
    "    label_encoder = LabelEncoder()\n",
    "    video_labels = torch.tensor(label_encoder.fit_transform(video_labels))\n",
    "    \n",
    "    # Print the mapping of original labels to numeric values\n",
    "    label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "    print(\"Label encoding mapping:\")\n",
    "    for label, numeric in label_mapping.items():\n",
    "        print(f\"{label}: {numeric}\")\n",
    "    \n",
    "    return video_features, video_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6318b2d-22d9-4f50-92f9-f966a9aa490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting and DataLoader Preparation\n",
    "# def prepare_data_loaders(video_features, video_labels, batch_size, test_size=0.2):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(video_features, video_labels, test_size=test_size, random_state=42)\n",
    "#     #X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "    \n",
    "#     # Create datasets\n",
    "#     train_dataset = TensorDataset(X_train, y_train)\n",
    "#     test_dataset = TensorDataset(X_test, y_test)\n",
    "#    # validation_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    \n",
    "#     # DataLoaders\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     #validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba18f70-8afa-425a-9190-54710cfe8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Function to apply SMOTE and prepare DataLoaders\n",
    "def prepare_data_loaders(video_features, video_labels, batch_size, test_size=0.2):\n",
    "    # Reshape the video features to 2D (samples, frames * features) for SMOTE\n",
    "    num_samples, num_frames, num_features = video_features.shape\n",
    "    reshaped_features = video_features.view(num_samples, -1)  # (samples, frames * features)\n",
    "    \n",
    "    # Apply SMOTE to balance the dataset\n",
    "    smote = SMOTE(random_state=42)\n",
    "    reshaped_features_np = reshaped_features.cpu().numpy()\n",
    "    video_labels_np = video_labels.cpu().numpy()\n",
    "    X_resampled, y_resampled = smote.fit_resample(reshaped_features_np, video_labels_np)\n",
    "    \n",
    "    # Reshape the resampled features back to 3D (samples, frames, features)\n",
    "    X_resampled_3d = torch.tensor(X_resampled).view(-1, num_frames, num_features)\n",
    "    y_resampled = torch.tensor(y_resampled)\n",
    "    \n",
    "    # Split the resampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled_3d, y_resampled, test_size=test_size, random_state=42)\n",
    "    # print(X_train.shape)\n",
    "    # print(y_train.shape)\n",
    "    # print(X_test.shape)\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75dad0b-2937-4891-83a6-99fe191bdee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for video_batch, labels in data_loader:\n",
    "        video_batch, labels = video_batch.to(device).float(), labels.to(device).long()\n",
    "       \n",
    "        # Ensure video_batch is in the correct dtype (float32)\n",
    "        #video_batch = video_batch.float()  # Convert to torch.float32\n",
    "\n",
    "        # Generate padding mask\n",
    "        padding_mask = generate_padding_mask(video_batch).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(video_batch, mask=padding_mask)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "    return model, avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9bbb49-d76c-41e5-966d-62d4266fb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for video_batch, labels in data_loader:\n",
    "            video_batch = video_batch.to(device).float()\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            # Generate padding mask (if applicable)\n",
    "            padding_mask = generate_padding_mask(video_batch).to(device)\n",
    "            #print(padding_mask)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(video_batch, mask=padding_mask)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = (correct_predictions / total_samples)\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793e599-0939-48c4-972b-7612d0f2e2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e703d98-b912-48f6-afdd-998fad2e5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# Updated cross-validation function\n",
    "def k_fold_cross_validation(model_class, video_features, video_labels, num_folds, batch_size, num_epochs, learning_rate, device):\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = []  # To store results for each fold\n",
    "    # all_embeddings = []  # To store embeddings for all folds\n",
    "    all_predicted = []\n",
    "    all_targets = []\n",
    "    trained_model = None \n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(video_features, video_labels)):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        # Split data into training and testing for this fold\n",
    "        X_train, X_test = video_features[train_idx], video_features[test_idx]\n",
    "        y_train, y_test = video_labels[train_idx], video_labels[test_idx]\n",
    "\n",
    "        # Apply SMOTE and prepare DataLoaders\n",
    "        train_loader, test_loader = prepare_data_loaders(X_train, y_train, batch_size)\n",
    "\n",
    "        model_fold = model_class.to(device)\n",
    "\n",
    "        # model_fold = VideoEmotionTransformer(\n",
    "        #     input_dim=input_dim, \n",
    "        #     hidden_dim=hidden_dim, \n",
    "        #     num_heads=num_heads, \n",
    "        #     num_layers=num_layers, \n",
    "        #     num_classes=num_classes, \n",
    "        #     dropout=dropout,\n",
    "        #     max_len=max_len\n",
    "        # ).to(device)\n",
    "\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model_fold.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training and testing loop for the fold\n",
    "        for epoch in range(num_epochs):\n",
    "            model_fold, avg_train_loss, accuracy_train = train_model(model_fold, train_loader, criterion, optimizer, device)\n",
    "            avg_test_loss, accuracy_test = test_model(model_fold, test_loader, criterion, device)\n",
    "            \n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Train Accuracy: {accuracy_train * 100:.2f}%, \"\n",
    "                  f\"Test Loss: {avg_test_loss:.4f}, \"\n",
    "                  f\"Test Accuracy: {accuracy_test * 100:.2f}%, \")\n",
    "\n",
    "        trained_model = model_fold \n",
    "\n",
    "        # # Collect embeddings and true labels for this fold\n",
    "        # embeddings, targets = get_embeddings(model_fold, test_loader, device)\n",
    "        # all_embeddings.extend(embeddings)\n",
    "        # all_targets.extend(targets)\n",
    "        \n",
    "        # Collect predictions and true labels for this fold\n",
    "        predicted, targets = get_predictions(model_fold, test_loader, device)\n",
    "        all_predicted.extend(predicted)\n",
    "        all_targets.extend(targets)\n",
    "\n",
    "        # Store results for this fold\n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'train_accuracy': accuracy_train,\n",
    "            'test_loss': avg_test_loss,\n",
    "            'test_accuracy': accuracy_test\n",
    "        })\n",
    "    \n",
    "    #After cross-validation, calculate and print average results across all folds\n",
    "    avg_train_loss = np.mean([res['train_loss'] for res in fold_results])\n",
    "    avg_train_accuracy = np.mean([res['train_accuracy'] for res in fold_results])\n",
    "    avg_test_loss = np.mean([res['test_loss'] for res in fold_results])\n",
    "    avg_test_accuracy = np.mean([res['test_accuracy'] for res in fold_results])\n",
    "    \n",
    "    print(\"\\nCross-validation results:\")\n",
    "    print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Average Train Accuracy: {avg_train_accuracy * 100:.2f}%\")\n",
    "    print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Average Test Accuracy: {avg_test_accuracy * 100:.2f}%\")\n",
    "    \n",
    "   # Calculate and print performance metrics for the whole cross-validation process\n",
    "    accuracy = accuracy_score(all_targets, all_predicted)\n",
    "    f1 = f1_score(all_targets, all_predicted, average='weighted')\n",
    "    report = classification_report(all_targets, all_predicted)\n",
    "    conf_matrix = confusion_matrix(all_targets, all_predicted)\n",
    "    \n",
    "    print(f\"\\nFinal Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Final F1 Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(video_labels)\n",
    "    class_labels = label_encoder.classes_\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    return trained_model #, fold_results\n",
    "\n",
    "#Helper function to get predictions from the model\n",
    "def get_predictions(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_predicted = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for video_batch, labels in test_loader:\n",
    "            # Move data to device and ensure correct dtype (float32)\n",
    "            video_batch = video_batch.to(device).float()  # Convert to float32\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Generate padding mask if needed\n",
    "            padding_mask = generate_padding_mask(video_batch).to(device)\n",
    "            \n",
    "            # Get the model outputs\n",
    "            outputs = model(video_batch, mask=padding_mask)\n",
    "            \n",
    "            # Get the predicted class\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return all_predicted, all_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45feb36-7620-4e5f-bbb2-ea10a27b7fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies, validation_losses=None, validation_accuracies=None):\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "    \n",
    "#     # Plotting Loss\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "#     plt.plot(test_losses, label='Test Loss', color='red')\n",
    "#     if validation_losses is not None:\n",
    "#         plt.plot(validation_losses, label='Validation Loss', color='orange')\n",
    "#     plt.title('Training and Test Loss')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Plotting Accuracy\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(train_accuracies, label='Train Accuracy', color='blue')\n",
    "#     plt.plot(test_accuracies, label='Test Accuracy', color='red')\n",
    "#     if validation_accuracies is not None:\n",
    "#         plt.plot(validation_accuracies, label='Validation Accuracy', color='orange')\n",
    "#     plt.title('Training and Test Accuracy')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a90c1c2-91a5-45fe-9c93-603fee917bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "num_folds = 5\n",
    "\n",
    "input_dim = 512  # Adjust according to your input features\n",
    "dropout = 0.3\n",
    "hidden_dim = 256  # Dimension for the hidden layers 256\n",
    "num_heads = 2     # Number of heads in the transformer 2\n",
    "num_layers = 4   # Number of transformer layers 2\n",
    "num_classes = 5   # Number of emotion classes\n",
    "max_len = 15      # Maximum sequence length (video frames)\n",
    "lr = 0.0001         # Learning rate #0.0001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77734b5a-ff49-46b3-8325-c5c360bd11ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding mapping:\n",
      "A: 0\n",
      "H: 1\n",
      "L: 2\n",
      "N: 3\n",
      "S: 4\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "import pandas as pd\n",
    "video_features, video_labels = preprocess_data(\"F:/thesis/Features/Final/Updated/Video_features_final.csv\", max_len=max_len, input_dim=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173dec52-4118-4b82-9c63-ec1a91c08116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8278, 15, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db3b38-36f8-4dfb-b4ea-028368b7b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Prepare DataLoaders\n",
    "#train_data_loader, test_loader = prepare_data_loaders(video_features, video_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0de988ca-548a-46e1-9b82-e044642395b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VideoEmotionRNN                          [16, 5]                   --\n",
       "├─Linear: 1-1                            [16, 15, 256]             131,328\n",
       "├─LSTM: 1-2                              [16, 15, 256]             2,105,344\n",
       "├─Linear: 1-3                            [16, 5]                   1,285\n",
       "├─Softmax: 1-4                           [16, 5]                   --\n",
       "==========================================================================================\n",
       "Total params: 2,237,957\n",
       "Trainable params: 2,237,957\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 507.40\n",
       "==========================================================================================\n",
       "Input size (MB): 0.49\n",
       "Forward/backward pass size (MB): 0.98\n",
       "Params size (MB): 8.95\n",
       "Estimated Total Size (MB): 10.43\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import torchinfo\n",
    "\n",
    "\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = VideoEmotionRNN(\n",
    "    input_dim=input_dim, \n",
    "    hidden_dim=hidden_dim,  \n",
    "    num_layers=num_layers, \n",
    "    num_classes=num_classes, \n",
    "    dropout = dropout\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "torchinfo.summary(model, input_size=(batch_size, max_len, input_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc1c67c-fc7b-4fe6-8533-20a6a237c0fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# K-fold cross-validation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_trained \u001b[38;5;241m=\u001b[39m \u001b[43mk_fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[1;34m(model_class, video_features, video_labels, num_folds, batch_size, num_epochs, learning_rate, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m video_labels[train_idx], video_labels[test_idx]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Apply SMOTE and prepare DataLoaders\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m model_fold \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# model_fold = VideoEmotionTransformer(\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#     input_dim=input_dim, \u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#     hidden_dim=hidden_dim, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Loss and optimizer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mprepare_data_loaders\u001b[1;34m(video_features, video_labels, batch_size, test_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data_loaders\u001b[39m(video_features, video_labels, batch_size, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(video_features, video_labels, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# K-fold cross-validation\n",
    "model_trained = k_fold_cross_validation(model, video_features, video_labels, num_folds=num_folds, batch_size=batch_size, num_epochs=num_epochs, learning_rate=lr, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d103b4-c98a-4b69-b1c3-0087913c61f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0fc20-148e-41cd-bcf3-1d8acfd0a6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2b782e1-2ed5-4a7b-88e4-94d0e1489730",
   "metadata": {},
   "source": [
    "### Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e53bef-8f4e-4edf-be08-0de362e239a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = model.to(device).float()  # Ensure model is in Float\n",
    "model_trained.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82573822-c954-4437-b651-c48ec535a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "video_features, video_labels = preprocess_data(\"F:/thesis/Features/Final/Updated/Video_features_final.csv\", max_len=max_len, input_dim=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e42ed-231b-4f23-8079-271dea194e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff07093-5e9c-40c9-b0b8-d7ee31c41266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting and DataLoader Preparation\n",
    "def prepare_data_loaders_emb(video_features, video_labels, batch_size):\n",
    "   \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(video_features, video_labels)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a703742-edb7-4e2d-baee-6adfe0335635",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Prepare DataLoaders\n",
    "data_loader = prepare_data_loaders_emb(video_features, video_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6273db-213e-4474-931d-53e51376a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients\n",
    "    for video_features, labels in data_loader:\n",
    "        video_features = video_features.to(device).float()  # Convert to Float\n",
    "        labels = labels.to(device)  # Ensure labels are also on the correct device\n",
    "\n",
    "        # Extract embeddings\n",
    "        embeddings = model_trained(video_features, extract_embeddings=True)\n",
    "\n",
    "        all_embeddings.append(embeddings.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "        all_labels.append(labels.cpu().numpy())  # Collect labels as well\n",
    "\n",
    "# Concatenate embeddings and labels across all batches\n",
    "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba95bb-96b7-464c-8c78-cdc0fd8d416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print results\n",
    "# print(\"\\nFold Results:\")\n",
    "# for result in fold_results:\n",
    "#     print(result)\n",
    "\n",
    "print(\"\\nExtracted Embeddings Shape:\", all_embeddings.shape)\n",
    "print(\"Targets Shape:\", all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e29629-9ad7-47a3-8fc9-36d2fd512740",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfddfd-1dd8-4c0e-899d-70a7e8610314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings and targets to numpy arrays if they are not already\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "all_targets = np.array(all_labels)\n",
    "\n",
    "# Ensure that embeddings and targets have the same number of samples\n",
    "assert all_embeddings.shape[0] == all_targets.shape[0], \"Mismatch between number of embeddings and targets\"\n",
    "\n",
    "# Convert embeddings to DataFrame\n",
    "embeddings_df = pd.DataFrame(all_embeddings)\n",
    "\n",
    "# Add the targets column\n",
    "embeddings_df['Target'] = all_targets\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "embeddings_df.to_csv('embeddings_with_targets.csv', index=False)\n",
    "\n",
    "print(\"\\nEmbeddings and targets have been saved to 'embeddings_with_targets.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f28b1-3bb3-494a-ad6a-fc03de3e3a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ac8d9-bbab-4411-8b07-b5e7dce8cc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
